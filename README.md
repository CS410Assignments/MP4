# MP4 Neural Ranking Methods

### Due Nov 11th,2022 at 11:59pm

In this MP you will explore how you can use Neural Network based methods for ranking for information retrieval. The goal of this assignment is to get you familiar with some popular approaches appling neural methods for information retrieval. For each of the three portions of the assigment you need to generate candidate rankings and a discussion where you evaluate performance and what factors may be impacting it.


## Retrieval Using Bi-Encoders and a Vector Database
While term based retireval based methods are effective they are commonly not effective at understanding semantic differences which are common in text. Bi encoders or dual encoders are a popular method because they are incredibly efficient. Using sentence transformers and FAISS you will search semantically on the CS410 document corpus. The goal in this portion of the assignement is to retrieve documents using a bi encoder and vector datasbase and produce a candidate ranking. Please submit a few candidate ranking and associated scores for the queries and collection found in the data folder. What differences in relevance do you see? how wide are the variations?
. Your outputed ranking should match the TREC format (#QUERY_ID\t0 DOCUMENTID\tRANK\tSCORE\trunid) and you can use the TRECEVAL notebook to explore how well diferent models perform. 

To help you explore how bi-encoders work and how they can be used with vector databases checkout ot the Bi-Encoder notebook. Details on how to format submissions and evaluate them can be found in the Evaluation notebook. To avoid long inference times we have gone ahead and generate a few sets of embeddings for the document and query corpus. If you want to explore further and look for other models they can be found in the [sentence transformer library](https://huggingface.co/sentence-transformers).
 
## Reranking using Cross Encoders
While bi-encoders can be effective they do not always produce the most relevant candidate sets. A common approach to improve this is to use a cross encoder to rerank the candidate sets generated by either bi-encoders or term based systems like BM25. A brief summary on how to use a cross encoder can be found in CrossEncoder notebook. In part 1 you generated ranking candidates using bi-encoders. Using that set, or the bm25-top1000 set found in the data folder, you will improve the ranking using a cross encoders. For this portion of the assigment you need to write a reranking function that uses a cross encoder to rerank a given set of documents and a query. Try out some of the cross encoders found in [the hub](https://huggingface.co/cross-encoder). How does performance differ? How does the size of the reranking set impact how well cross encoders work? 

## Hybrid Search With ReRanking
Using what you have learned about bi-encoders, cross encoders, and term based search you will bring it all together. Write a ranking function that combines the dense search from bi-encoders with the sparse search from BM25 which you rerank with one or many cross encoders. Please describe you implementation along with the code for the implementation and results. 

### Tips and Suggestions
To learn more about the how to use ANN retrieval we suggest checking out [this demo](https://github.com/facebookresearch/faiss/wiki/Getting-started). Another great resource is [Pinecone](https://www.pinecone.io/learn/faiss-tutorial/)

To learn more about the models which can be used for semantic search check out [SBERT](https://www.sbert.net/)

To learn more about TREC Eval and the ir-measure wrapper check out [their website](https://ir-measur.es/en/latest/getting-started.html#run-formats)

The BM25 candidate set was generated using [pyserini](https://github.com/castorini/pyserini). It is one of the most popular python based search libraries out there. If you want to explore any experiments or how ranking methods work together check out the examples in their repo.