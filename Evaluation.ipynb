{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sufficient-lottery",
   "metadata": {},
   "source": [
    "# Evaluating Search Sytems using TREC EVAL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-louisiana",
   "metadata": {},
   "source": [
    "This contains an example on how to go ahead and evaluate models using ir-measures, which is a wrapper on the popular TRECEVAL framework. TRECEVAL is the common framework for format for evaluating search sysems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "activated-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_measures\n",
    "from ir_measures import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "architectural-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trec(filename):\n",
    "    qid2docid = {}\n",
    "    with open(filename,'r') as f:\n",
    "        for l in f:\n",
    "            l = l.strip().split('\\t')\n",
    "            qid = int(l[0])\n",
    "            doc_id = int(l[2])\n",
    "            rank = int(l[3])\n",
    "            score = float(l[4])\n",
    "            if qid not in qid2docid:\n",
    "                qid2docid[qid] = {}\n",
    "            qid2docid[qid][rank] = (doc_id,score)\n",
    "    return qid2docid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-maria",
   "metadata": {},
   "source": [
    "The example shown below will load the ranking, write it out in TREC format and then evaluate them on the previously generate relevance labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fatal-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_candidates = load_trec('data/bm25-top1000.trec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mature-being",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1936, 6.0995)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates[1][1] # First value is the doc_id and the second is score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "distinct-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "top_n = 10 # keep top 10 in ranking\n",
    "with open('ranking_candidate.trec','w') as w:\n",
    "    for i in range(1,len(bm25_candidates)):\n",
    "        i += 1 \n",
    "        if i in bm25_candidates:\n",
    "            candidates = bm25_candidates[i]\n",
    "            for j in range(1,min(len(candidates),(top_n+1))): #rank starts at 1\n",
    "                w.write(\"{}\\t0\\t{}\\t{}\\t{}\\tcandidate-run\\n\".format(i, candidates[j][0],j, candidates[j][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "parental-evans",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (1088, 6.8592),\n",
       " 2: (2328, 6.2865),\n",
       " 3: (2323, 6.2074),\n",
       " 4: (3608, 5.33),\n",
       " 5: (6353, 4.3785),\n",
       " 6: (368, 4.3106),\n",
       " 7: (6836, 4.283),\n",
       " 8: (6350, 4.1913),\n",
       " 9: (3503, 4.0519),\n",
       " 10: (1717, 3.8778),\n",
       " 11: (1716, 3.7843),\n",
       " 12: (762, 3.7802),\n",
       " 13: (1085, 3.774),\n",
       " 14: (1689, 3.748),\n",
       " 15: (99, 3.7199),\n",
       " 16: (2440, 3.6997),\n",
       " 17: (400, 3.6818),\n",
       " 18: (398, 3.6644),\n",
       " 19: (2321, 3.5959),\n",
       " 20: (5154, 3.5922),\n",
       " 21: (1584, 3.5557),\n",
       " 22: (402, 3.5557),\n",
       " 23: (403, 3.5557),\n",
       " 24: (1715, 3.5528),\n",
       " 25: (2092, 3.5315),\n",
       " 26: (6946, 3.3858),\n",
       " 27: (342, 3.376),\n",
       " 28: (5158, 3.374),\n",
       " 29: (557, 3.373),\n",
       " 30: (98, 3.3662),\n",
       " 31: (6295, 3.35),\n",
       " 32: (366, 3.3373),\n",
       " 33: (766, 3.3278),\n",
       " 34: (339, 3.3231),\n",
       " 35: (395, 3.2996),\n",
       " 36: (173, 3.281),\n",
       " 37: (2327, 3.281),\n",
       " 38: (6437, 3.1653),\n",
       " 39: (2443, 3.0505),\n",
       " 40: (1744, 3.0233),\n",
       " 41: (3037, 2.9287),\n",
       " 42: (6650, 2.8181),\n",
       " 43: (3609, 2.8046),\n",
       " 44: (3611, 2.8046),\n",
       " 45: (146, 2.7979),\n",
       " 46: (3764, 2.7979),\n",
       " 47: (3767, 2.7979),\n",
       " 48: (4558, 2.7979),\n",
       " 49: (4560, 2.7979),\n",
       " 50: (6708, 2.79789),\n",
       " 51: (1480, 2.7912),\n",
       " 52: (4026, 2.7912),\n",
       " 53: (853, 2.7845),\n",
       " 54: (137, 2.7713),\n",
       " 55: (396, 2.7452),\n",
       " 56: (4422, 2.705),\n",
       " 57: (367, 2.7007),\n",
       " 58: (5785, 2.6759),\n",
       " 59: (2987, 2.6576),\n",
       " 60: (340, 2.6396),\n",
       " 61: (2263, 2.6159),\n",
       " 62: (6503, 2.5812),\n",
       " 63: (3758, 2.5642),\n",
       " 64: (2923, 2.5198),\n",
       " 65: (2940, 2.4876),\n",
       " 66: (6423, 2.4876),\n",
       " 67: (6355, 2.4236),\n",
       " 68: (2377, 2.4055),\n",
       " 69: (3494, 2.3935),\n",
       " 70: (1585, 2.3474),\n",
       " 71: (415, 2.3474),\n",
       " 72: (2326, 2.2222),\n",
       " 73: (6721, 2.1888),\n",
       " 74: (1790, 2.1565),\n",
       " 75: (3480, 2.0233),\n",
       " 76: (5285, 2.008),\n",
       " 77: (4118, 1.9169),\n",
       " 78: (6881, 1.9031),\n",
       " 79: (2763, 1.8547),\n",
       " 80: (4079, 1.8547),\n",
       " 81: (5171, 1.8547),\n",
       " 82: (5655, 1.8547),\n",
       " 83: (5084, 1.7232),\n",
       " 84: (6871, 1.4209),\n",
       " 85: (6872, 1.4209),\n",
       " 86: (990, 1.3676),\n",
       " 87: (549, 1.2858),\n",
       " 88: (556, 1.2858),\n",
       " 89: (2343, 1.2831),\n",
       " 90: (1554, 1.2563),\n",
       " 91: (2972, 1.2385),\n",
       " 92: (1173, 1.1516),\n",
       " 93: (6451, 1.0831),\n",
       " 94: (1574, 1.0502),\n",
       " 95: (555, 1.0502),\n",
       " 96: (6359, 0.9681),\n",
       " 97: (6453, 0.9193),\n",
       " 98: (3698, 0.835),\n",
       " 99: (5630, 0.7952),\n",
       " 100: (4124, 0.7341),\n",
       " 101: (3832, 0.7145),\n",
       " 102: (6482, 0.7136),\n",
       " 103: (6405, 0.6794),\n",
       " 104: (1745, 0.6224),\n",
       " 105: (1154, 0.5912),\n",
       " 106: (3400, 0.5352),\n",
       " 107: (2446, 0.1208)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "headed-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels = ir_measures.read_trec_qrels('data/qrels.trec')\n",
    "run = ir_measures.read_trec_run('ranking_candidate.trec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "analyzed-flooring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{nDCG@20: 0.4077845211129665, nDCG@5: 0.49344206152248493, R@100: 0.3636096868298368, R@10: 0.3636096868298368, nDCG@10: 0.4712068892026146, R@1000: 0.3636096868298368, nDCG@3: 0.5161010912381276}\n"
     ]
    }
   ],
   "source": [
    "print(ir_measures.calc_aggregate([NDCG@5,NDCG@10,NDCG@3,NDCG@20,R@10,R@100,R@1000,], qrels, run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-survey",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
